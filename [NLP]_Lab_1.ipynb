{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/18101243dhehun/NLP/blob/main/%5BNLP%5D_Lab_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sF0BmaZ-1zmk"
      },
      "source": [
        "#Analyzing Text Data\n",
        "## Load relevent Packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "lG40Q87R1zmm"
      },
      "outputs": [],
      "source": [
        "# Import spacy\n",
        "import spacy\n",
        "\n",
        "# Loading NLTK module\n",
        "import nltk\n",
        "\n",
        "# downloading punkt\n",
        "nltk.download('punkt')\n",
        "\n",
        "# downloading stopwords\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# downloading omw-1.4\n",
        "nltk.download('omw-1.4')\n",
        "\n",
        "# downloading wordnet\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# downloading average_perception_tagger \n",
        "nltk.download('averaged_perceptron_tagger')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hFLNnmbs1zmm"
      },
      "source": [
        "## Tokenization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "YTtW6Iuq1zmn"
      },
      "outputs": [],
      "source": [
        "# Sentence Tokenization\n",
        "from nltk.tokenize import sent_tokenize\n",
        " \n",
        "paragraph=\"\"\"Taj Mahal is one of the beautiful monuments. It is one of the wonders of the world. It was built by Shah Jahan in 1631 in memory of his third beloved wife Mumtaj Mahal.\"\"\"\n",
        " \n",
        "tokenized_sentences=sent_tokenize(paragraph)\n",
        "print(tokenized_sentences)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "HiuBT-jm1zmn"
      },
      "outputs": [],
      "source": [
        "# Loading english language model \n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        " \n",
        "# Append the sentencizer pipe to the nlp pipeline\n",
        "nlp.add_pipe('sentencizer')\n",
        " \n",
        "paragraph = \"\"\"Taj Mahal is one of the beautiful monuments. It is one of the wonders of the world. It was built by Shah Jahan in 1631 in memory of his third beloved wife Mumtaj Mahal.\"\"\"\n",
        " \n",
        "# Create nlp Object to handle linguistic annotations in a documents.\n",
        "nlp_doc = nlp(paragraph)\n",
        " \n",
        "# Generate list of tokenized sentence\n",
        "tokenized_sentences = []\n",
        "for sentence in nlp_doc.sents:\n",
        "    tokenized_sentences.append(sentence.text)\n",
        "print(tokenized_sentences)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "nscR6b8g1zmn"
      },
      "outputs": [],
      "source": [
        "# Import nltk word_tokenize method\n",
        "from nltk.tokenize import word_tokenize\n",
        " \n",
        "# Split paragraph into words\n",
        "tokenized_words=word_tokenize(paragraph)\n",
        "print(tokenized_words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "2xk4p7D_1zmo"
      },
      "outputs": [],
      "source": [
        "# Loading english language model \n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        " \n",
        "paragraph = \"\"\"Taj Mahal is one of the beautiful monuments. It is one of the wonders of the world. It was built by Shah Jahan in 1631 in memory of his third beloved wife Mumtaj Mahal.\"\"\"\n",
        " \n",
        "# Create nlp Object to handle linguistic annotations in a documents.\n",
        "my_doc = nlp(paragraph)\n",
        " \n",
        "# tokenize paragraph into words\n",
        "tokenized_words = []\n",
        "for token in my_doc:\n",
        "    tokenized_words.append(token.text)\n",
        "print(tokenized_words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "a_L70u5y1zmo"
      },
      "outputs": [],
      "source": [
        "# Import frequency distribution  \n",
        "from nltk.probability import FreqDist\n",
        " \n",
        "# Find frequency distribution of paragraph\n",
        "fdist = FreqDist(tokenized_words)\n",
        " \n",
        "# Check top 5 common words\n",
        "fdist.most_common(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "ZbB7o_911zmp"
      },
      "outputs": [],
      "source": [
        "# Import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        " \n",
        "# Plot Frequency Distribution\n",
        "fdist.plot(20, cumulative=False)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CCPSXWQG1zmp"
      },
      "source": [
        "## Stopwords"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "xnKZtTg8cESS"
      },
      "outputs": [],
      "source": [
        "# import the nltk stopwords \n",
        "from nltk.corpus import stopwords\n",
        " \n",
        "# Load english stopwords list\n",
        "stopwords_set=set(stopwords.words(\"english\"))\n",
        " \n",
        "print(stopwords_set)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "0GrCxLbs1zmp"
      },
      "outputs": [],
      "source": [
        "# Removing stopwords from text\n",
        "filtered_word_list=[]\n",
        "for word in tokenized_words:\n",
        "    # filter stopwords\n",
        "    if word not in stopwords_set:\n",
        "        filtered_word_list.append(word)\n",
        "\n",
        "# print tokenized words\n",
        "print(\"Tokenized Word List:\", tokenized_words)\n",
        "\n",
        "# print filtered words\n",
        "print(\"Filtered Word List:\", filtered_word_list)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "9UXt8xVn1zmp"
      },
      "outputs": [],
      "source": [
        "# Loading english language model \n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        " \n",
        "# text paragraph\n",
        "paragraph = \"\"\"Taj Mahal is one of the beautiful monuments. It is one of the wonders of the world. It was built by Shah Jahan in 1631 in memory of his third beloved wife Mumtaj Mahal.\"\"\"\n",
        "\n",
        "# Create nlp Object to handle linguistic annotations in a documents.\n",
        "my_doc = nlp(paragraph)\n",
        " \n",
        "# Removing stopwords from text\n",
        "filtered_token_list=[]\n",
        "for token in my_doc:\n",
        "    # filter stopwords \n",
        "    if token.is_stop==False:\n",
        "        filtered_token_list.append(token)\n",
        "        \n",
        "print(\"Filtered Word List:\",filtered_token_list)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f5Yn-XIQ1zmp"
      },
      "source": [
        "## Stemming and Lemmatization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "veOsxXzU1zmq"
      },
      "outputs": [],
      "source": [
        "# Import Lemmatizer\n",
        "from nltk.stem.wordnet import WordNetLemmatizer\n",
        "\n",
        "# Create lemmatizer object\n",
        "lemmatizer = WordNetLemmatizer()\n",
        " \n",
        "# Import Porter Stemmer\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "\n",
        "# Create stemmer object\n",
        "stemmer = PorterStemmer()\n",
        "\n",
        "# take a sample word\n",
        "sample_word = \"crying\"\n",
        "\n",
        "print(\"Lemmatized Sample Word:\", lemmatizer.lemmatize(sample_word,\"v\"))\n",
        "\n",
        "print(\"Stemmed Sample Word:\", stemmer.stem(sample_word))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "Id0PDtjZ1zmq"
      },
      "outputs": [],
      "source": [
        "# Loading english language model \n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        " \n",
        "# Create nlp Object to handle linguistic annotations in documents.\n",
        "words = nlp(\"cry cries crying\")\n",
        " \n",
        "# Find lemmatized word\n",
        "for w in words:\n",
        "    print('Original Word: ', w.text)\n",
        "    print('Lemmatized Word: ',w.lemma_)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QxenE1_A1zmq"
      },
      "source": [
        "## PoS Tagging"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "rvCoT9mw1zmq"
      },
      "outputs": [],
      "source": [
        "# import Word Tokenizer and Pos Tagger\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk import pos_tag\n",
        " \n",
        "# Sample sentence\n",
        "sentence = \"Taj Mahal is one of the beautiful monument.\"\n",
        " \n",
        "# Tokenize the sentence\n",
        "sent_tokens = word_tokenize(sentence)\n",
        " \n",
        "# Create PoS tags\n",
        "sent_pos = pos_tag(sent_tokens)\n",
        " \n",
        "# Print tokens with PoS\n",
        "print(sent_pos)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "DW9X8Iuv1zmq"
      },
      "outputs": [],
      "source": [
        "# Loading small english language model \n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Create nlp Object to handle linguistic annotations in a documents.\n",
        "sentence = nlp(u\"Taj Mahal is one of the beautiful monument.\")\n",
        " \n",
        "for token in sentence:\n",
        "    print(token.text, token.pos_)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5HvnsV5k1zmq"
      },
      "source": [
        "## Entity Recognition"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "FjmhRWq-1zmq"
      },
      "outputs": [],
      "source": [
        "# Load English model for tokenizer, tagger, parser, and NER \n",
        "nlp = spacy.load('en_core_web_sm') \n",
        " \n",
        "# Sample paragraph\n",
        "paragraph = \"\"\"Taj Mahal is one of the beautiful monuments. It is one of the wonders of the world. It was built by Shah Jahan in 1631 in memory of his third beloved wife Mumtaj Mahal.\"\"\"\n",
        " \n",
        "# Create nlp Object to handle linguistic annotations in documents.\n",
        "docs=nlp(paragraph)\n",
        " \n",
        "entities=[(i.text, i.label_) for i in docs.ents]\n",
        "print(entities)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "0CR6cxSp1QEV"
      },
      "outputs": [],
      "source": [
        "# Import display for visualizing the Entities\n",
        "from spacy import displacy\n",
        " # Visualize the entities using render function\n",
        "displacy.render(docs, style = \"ent\",jupyter = True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q2H49yd71zmq"
      },
      "source": [
        "## Dependency Parsing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "83iL1YY21zmr"
      },
      "outputs": [],
      "source": [
        "# Load English model for tokenizer, tagger, parser, and NER \n",
        "nlp = spacy.load('en_core_web_sm') \n",
        " \n",
        "# Sample sentence\n",
        "sentence=\"Taj Mahal is one of the beautiful monument.\"\n",
        "\n",
        "# Create nlp Object to handle linguistic annotations in a documents.\n",
        "docs=nlp(sentence)\n",
        " \n",
        "# Visualize the  using render function\n",
        "displacy.render(docs, style=\"dep\", jupyter= True, options={'distance': 150})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xy-xuSZH1zmr"
      },
      "source": [
        "## WordCloud"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "7iBLmtMA1zmr"
      },
      "outputs": [],
      "source": [
        "# importing all necessary modules \n",
        "from wordcloud import WordCloud\n",
        "from wordcloud import STOPWORDS\n",
        "import matplotlib.pyplot as plt\n",
        " \n",
        "stopword_list = set(STOPWORDS) \n",
        " \n",
        "paragraph=\"\"\"Taj Mahal is one of the beautiful monuments. It is one of the wonders of the world. It was built by Shah Jahan in 1631 in memory of his third beloved wife Mumtaj Mahal.\"\"\"\n",
        " \n",
        "word_cloud = WordCloud(width = 550, height = 550, \n",
        "                       background_color ='white', \n",
        "                       stopwords = stopword_list, \n",
        "                       min_font_size = 10).generate(paragraph) \n",
        "  \n",
        "# Visualize the WordCloud Plot\n",
        "\n",
        "# Set wordcloud figure size\n",
        "plt.figure(figsize = (8, 6)) \n",
        "# Show image\n",
        "plt.imshow(word_cloud) \n",
        "# Remove Axis\n",
        "plt.axis(\"off\")  \n",
        "# show plot\n",
        "plt.show() "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CVv92oPl1zmr"
      },
      "source": [
        "## Sentiment Analysis using Text Classification\n",
        "### Classification using Bag of Words (or TF)\n",
        "## Load the Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "XGG2NDL_1zmr"
      },
      "outputs": [],
      "source": [
        "# Import libraries\n",
        "import pandas as pd\n",
        "\n",
        "# read the dataset\n",
        "copus_url = 'https://raw.githubusercontent.com/sharmaroshan/Amazon-Alexa-Reviews/master/amazon_alexa.tsv'\n",
        "df=pd.read_csv(copus_url, sep='\\t')\n",
        "    \n",
        "# Show top 5-records\n",
        "df.tail()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0NSOzuOF1zmr"
      },
      "source": [
        "#### Explore the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "InFRWbIz1zmr"
      },
      "outputs": [],
      "source": [
        "# Import seaborn\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        " \n",
        "# Count plot\n",
        "sns.countplot(x='feedback', data=df)\n",
        " \n",
        "# Set X-axis and Y-axis labels\n",
        "plt.xlabel('Sentiment Score')\n",
        "plt.ylabel('Number of Records')\n",
        " \n",
        "# Show the plot using show() function\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IvF42p7b1zmr"
      },
      "source": [
        "#### Feature Generation using CountVectorizer "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "d5vsIaRz1zmr"
      },
      "outputs": [],
      "source": [
        "# Import CountVectorizer and RegexTokenizer\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        " \n",
        "# Create Regex tokenizer for removing special symbols and numeric values \n",
        "regex_tokenizer = RegexpTokenizer(r'[a-zA-Z]+')\n",
        " \n",
        "# Initialize CountVectorizer object\n",
        "count_vectorizer = CountVectorizer(lowercase=True, \n",
        "                     stop_words='english', \n",
        "                     ngram_range = (1,1), \n",
        "                     tokenizer = regex_tokenizer.tokenize)\n",
        " \n",
        "# Fit and transform the dataset\n",
        "count_vectors = count_vectorizer.fit_transform(df['verified_reviews'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "17JCjcdh1zmr"
      },
      "source": [
        "#### Split train and test set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "fKmG3rjW1zms"
      },
      "outputs": [],
      "source": [
        "# Import train_test_split\n",
        "from sklearn.model_selection import train_test_split\n",
        " \n",
        "# Partition data into training and testing set \n",
        "from sklearn.model_selection import train_test_split\n",
        "feature_train, feature_test, target_train, target_test = train_test_split(\n",
        "    count_vectors, df['feedback'], test_size=0.3, random_state=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IL34MmhZ1zms"
      },
      "source": [
        "#### Classification Model Building using Logistic Regression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "Ko12mlSl1zms"
      },
      "outputs": [],
      "source": [
        "# import logistic regression scikit-learn model\n",
        "from sklearn.linear_model import LogisticRegression\n",
        " \n",
        "# instantiate the model\n",
        "logreg = LogisticRegression(solver='lbfgs')\n",
        " \n",
        "# fit the model with data\n",
        "logreg.fit(feature_train,target_train)\n",
        " \n",
        "# Forecast the target variable for given test dataset\n",
        "predictions = logreg.predict(feature_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a_wIcYtI1zms"
      },
      "source": [
        "#### Evaluate the Classification Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "jt2yFf8a1zms",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# Import metrics module for performance evaluation\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import precision_score\n",
        "from sklearn.metrics import recall_score\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "# Assess model performance using accuracy measure\n",
        "print(\"Logistic Regression Model Accuracy:\",accuracy_score(target_test, predictions))\n",
        "# Calculate model precision\n",
        "print(\"Logistic Regression Model Precision:\",precision_score(target_test, predictions))\n",
        "# Calculate model recall\n",
        "print(\"Logistic Regression Model Recall:\",recall_score(target_test, predictions))\n",
        "# Calculate model f1 score\n",
        "print(\"Logistic Regression Model F1-Score:\",f1_score(target_test, predictions))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o9MkTVPo1zms"
      },
      "source": [
        "### Classification using TF-IDF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "737UdMvu1zms"
      },
      "outputs": [],
      "source": [
        "# Import TfidfVectorizer and RegexTokenizer\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Create Regex tokenizer for removing special symbols and numeric values \n",
        "regex_tokenizer = RegexpTokenizer(r'[a-zA-Z]+')\n",
        " \n",
        "# Initialize TfidfVectorizer object\n",
        "tfidf = TfidfVectorizer(lowercase=True,stop_words='english',ngram_range = (1,1),tokenizer = regex_tokenizer.tokenize)\n",
        " \n",
        "# Fit and transform the dataset\n",
        "text_tfidf= tfidf.fit_transform(df['verified_reviews'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "9JtyZbrs1zms"
      },
      "outputs": [],
      "source": [
        "# Import train_test_split\n",
        "from sklearn.model_selection import train_test_split\n",
        " \n",
        "# Partition data into training and testing set \n",
        "from sklearn.model_selection import train_test_split\n",
        "feature_train, feature_test, target_train, target_test = train_test_split(\n",
        "    text_tfidf, df['feedback'], test_size=0.3, random_state=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "ylg9tJu21zms"
      },
      "outputs": [],
      "source": [
        "# import logistic regression scikit-learn model\n",
        "from sklearn.linear_model import LogisticRegression\n",
        " \n",
        "# instantiate the model\n",
        "logreg = LogisticRegression(solver='lbfgs')\n",
        " \n",
        "# fit the model with data\n",
        "logreg.fit(feature_train,target_train)\n",
        " \n",
        "# Forecast the target variable for given test dataset\n",
        "predictions = logreg.predict(feature_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "g3h2OeW_1zms"
      },
      "outputs": [],
      "source": [
        "# Import metrics module for performance evaluation\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import precision_score\n",
        "from sklearn.metrics import recall_score\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "# Assess model performance using accuracy measure\n",
        "print(\"Logistic Regression Model Accuracy:\",accuracy_score(target_test, predictions))\n",
        "# Calculate model precision\n",
        "print(\"Logistic Regression Model Precision:\",precision_score(target_test, predictions))\n",
        "# Calculate model recall\n",
        "print(\"Logistic Regression Model Recall:\",recall_score(target_test, predictions))\n",
        "# Calculate model f1 score\n",
        "print(\"Logistic Regression Model F1-Score:\",f1_score(target_test, predictions))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y2PVAjSw1zms"
      },
      "source": [
        "## Text Similarity"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "dHZG4VPv1zms"
      },
      "outputs": [],
      "source": [
        "def jaccard_similarity(sent1, sent2):\n",
        "    \"\"\"Find text similarity using jaccard similarity\"\"\"\n",
        "    \n",
        "    # Tokenize sentences\n",
        "    token1 = set(sent1.split())\n",
        "    token2 = set(sent2.split())\n",
        "     \n",
        "    # intersection between tokens of two sentences    \n",
        "    intersection_tokens = token1.intersection(token2)\n",
        "    \n",
        "    # Union between tokens of two sentences\n",
        "    union_tokens=token1.union(token2)\n",
        "    \n",
        "    # Cosine Similarity\n",
        "    sim_= float(len(intersection_tokens) / len(union_tokens))\n",
        "    return sim_\n",
        "\n",
        "# Call function\n",
        "jaccard_similarity('I love pets.','I hate pets.')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "i7jd19bJ1zms"
      },
      "outputs": [],
      "source": [
        "# Let's import text feature extraction TfidfVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        " \n",
        "docs=['I love pets.','I hate pets.']\n",
        " \n",
        "# Initialize TfidfVectorizer object\n",
        "tfidf= TfidfVectorizer()\n",
        " \n",
        "# Fit and transform the given data\n",
        "tfidf_vector = tfidf.fit_transform(docs)\n",
        " \n",
        "# Import cosine_similarity metrics\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        " \n",
        "# compute similarity using cosine similarity\n",
        "cos_sim=cosine_similarity(tfidf_vector[0], tfidf_vector[1])\n",
        "print(cos_sim)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "Tgq2J3yG82sv"
      },
      "outputs": [],
      "source": [
        "# Load English model for tokenizer, tagger, parser, and NER \n",
        "nlp = spacy.load('en_core_web_sm') \n",
        " \n",
        "# Create documents 123123123\n",
        "doc1 = nlp(u'I love pets.')\n",
        "doc2 = nlp(u'I hate pets')\n",
        " \n",
        "# Find similarity\n",
        "print(doc1.similarity(doc2))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "ULzTxiWY8QmP"
      },
      "outputs": [],
      "source": [
        "!python -m spacy download en_core_web_lg"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "JSse2W-W1zms"
      },
      "outputs": [],
      "source": [
        "# Import spacy\n",
        "import spacy\n",
        " \n",
        "# Load English model for tokenizer, tagger, parser, and NER \n",
        "nlp = spacy.load('en_core_web_lg') \n",
        " \n",
        "# Create documents\n",
        "doc1 = nlp(u'I love pets.')\n",
        "doc2 = nlp(u'I hate pets')\n",
        " \n",
        "# Find similarity\n",
        "print(doc1.similarity(doc2))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SMRL_eKmL43W"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "# [Lap 1] Sentiment Analysis!\n",
        "Try Sentiment Analysis by following suggenstions.\n",
        "- Use Other dataset: [dataset list](https://research.aimultiple.com/sentiment-analysis-dataset/)\n",
        "- Change your classifier: Other than `Logistic Regression`. [ref](https://www.kaggle.com/code/jeffd23/10-classifier-showdown-in-scikit-learn)\n",
        "- Additional preprocessing: n-gram, stopwords [ref](https://towardsdatascience.com/nlp-preprocessing-with-nltk-3c04ee00edc0)\n",
        "\n",
        "Will grade it only on and off. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 121,
      "metadata": {
        "id": "X_Zys5FnwyVt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ab5acf88-d956-4a32-a745-4c557a489b08"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ibXb6FnIVjsb"
      },
      "source": [
        "## Load the Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I5RPkUX9OJFE"
      },
      "outputs": [],
      "source": [
        "# read the dataset\n",
        "df = pd.read_csv('/content/drive/MyDrive/Movie_Review.csv', sep=',')\n",
        "\n",
        "    \n",
        "# Show top 5-records\n",
        "df.tail()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0WcvKXDEVxM_"
      },
      "source": [
        "#### Explore the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y1E3y0ACVx5e"
      },
      "outputs": [],
      "source": [
        "# Count plot\n",
        "sns.countplot(x='label', data=df)\n",
        " \n",
        "# Set X-axis and Y-axis labels\n",
        "plt.xlabel('Sentiment Score')\n",
        "plt.ylabel('Number of Records')\n",
        " \n",
        "# Show the plot using show() function\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_rDidrUOWlSI"
      },
      "source": [
        "Feature Generation using CountVectorizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FVFlv9Q8WmDG"
      },
      "outputs": [],
      "source": [
        "# Create Regex tokenizer for removing special symbols and numeric values \n",
        "regex_tokenizer = RegexpTokenizer(r'[a-zA-Z]+')\n",
        " \n",
        "# Initialize CountVectorizer object\n",
        "count_vectorizer = CountVectorizer(lowercase=True, \n",
        "                     stop_words='english', \n",
        "                     ngram_range = (1,1), \n",
        "                     tokenizer = regex_tokenizer.tokenize)\n",
        " \n",
        "# Fit and transform the dataset\n",
        "count_vectors = count_vectorizer.fit_transform(df['text'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lobgnrtrY4s9"
      },
      "source": [
        "#### Split train and test set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nqGkV-oXY62l"
      },
      "outputs": [],
      "source": [
        "feature_train, feature_test, target_train, target_test = train_test_split(\n",
        "    count_vectors, df['label'], test_size=0.3, random_state=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o0lg29BNZq72"
      },
      "source": [
        "#### Classification Model Building using KNeighborsClassifier\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kt-VfUJvZsP-"
      },
      "outputs": [],
      "source": [
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.svm import SVC\n",
        "\n",
        " \n",
        "# instantiate the model\n",
        "svc = SVC(kernel='linear')\n",
        " \n",
        "# fit the model with data\n",
        "svc.fit(feature_train,target_train)\n",
        " \n",
        "# Forecast the target variable for given test dataset\n",
        "predictions = svc.predict(feature_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f4VuT3TnaUAz"
      },
      "source": [
        "#### Evaluate the Classification Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 123,
      "metadata": {
        "id": "pdRc9d29aYZe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "63be03d8-2add-4144-b02e-85f8226d2c5e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SVC Model Accuracy: 0.9238095238095239\n",
            "SVC Model Precision: 0.923728813559322\n",
            "SVC Model Recall: 1.0\n",
            "SVC Model F1-Score: 0.960352422907489\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Assess model performance using accuracy measure\n",
        "print(\"SVC Model Accuracy:\",accuracy_score(target_test, predictions))\n",
        "# Calculate model precision\n",
        "print(\"SVC Model Precision:\",precision_score(target_test, predictions))\n",
        "# Calculate model recall\n",
        "print(\"SVC Model Recall:\",recall_score(target_test, predictions))\n",
        "# Calculate model f1 score\n",
        "print(\"SVC Model F1-Score:\",f1_score(target_test, predictions))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}